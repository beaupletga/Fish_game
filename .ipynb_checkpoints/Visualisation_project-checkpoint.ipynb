{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL.Image import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy import misc\n",
    "import time\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import operator\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1286, 372)\n",
      "(1, 1173, 212)\n",
      "(224, 1173, 212)\n",
      "(285, 1173, 212)\n",
      "(224, 1173, 212)\n"
     ]
    }
   ],
   "source": [
    "list_dir=[]\n",
    "list_shapes=[]\n",
    "# we iterate over the folder fishes which contains the ct scan of each fish\n",
    "root_adress=\"Fishes/\"\n",
    "for i in next(os.walk(root_adress))[1]:\n",
    "    if i!=\".ipynb_checkpoints\":\n",
    "        list_dir.append(root_adress+i)\n",
    "        \n",
    "for i in list_dir:\n",
    "    first_file=np.expand_dims(misc.imread(i+'/'+os.listdir(i)[0])[:,:,0],0)\n",
    "    list_shapes.append(first_file.shape)\n",
    "    print(first_file.shape)\n",
    "\n",
    "# we get the minimum shape for each axis in order to put all the volume at the same scale (the minimum one)   \n",
    "shapes0=[len(os.listdir(i)) for i in list_dir]    \n",
    "min_shape0=min(shapes0)\n",
    "min_shape1=min(list_shapes,key= lambda x:x[1])[1]\n",
    "min_shape2=min(list_shapes,key= lambda x:x[2])[2]\n",
    "print(min_shape0,min_shape1,min_shape2)\n",
    "\n",
    "\n",
    "for it,repo in enumerate(list_dir):\n",
    "    files_list=sorted(os.listdir(repo))\n",
    "    files_list=[files_list[i] for i in range(len(files_list)) if i%int(shapes0[it]/min_shape0)==0]\n",
    "    images_list=[np.expand_dims(misc.imread(repo+\"/\"+f)[:,:,0],0) for f in files_list]\n",
    "    volume=np.concatenate(images_list)\n",
    "    volume=volume[:,np.arange(0,volume.shape[1],int(volume.shape[1]/min_shape1)),:]\n",
    "    volume=volume[:,:min_shape1,:]\n",
    "    volume=volume[:,:,np.arange(0,volume.shape[2],int(volume.shape[2]/min_shape2))]\n",
    "    volume=volume[:,:,:min_shape2]\n",
    "    print(volume.shape)\n",
    "    if it==0:\n",
    "        whole_volume=volume\n",
    "    else:\n",
    "        whole_volume=np.concatenate((whole_volume,volume),axis=0)\n",
    "volume=whole_volume.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we downsample the whole volume\n",
    "whole_volume=whole_volume[np.arange(0,whole_volume.shape[0],8),:,:]\n",
    "whole_volume=whole_volume[:,np.arange(0,whole_volume.shape[1],8),:]\n",
    "whole_volume=whole_volume[:,:,np.arange(0,whole_volume.shape[2],8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "# return the indices of the adjacents vertices\n",
    "def get_neighbors(int x, int y, int z,np.ndarray array,factor):\n",
    "    index_list=[]\n",
    "    cdef int i=0;\n",
    "    cdef int j=0;\n",
    "    cdef int k=0;\n",
    "    cdef int shape0=array.shape[0]\n",
    "    cdef int shape1=array.shape[1]\n",
    "    cdef int shape2=array.shape[2]\n",
    "    for i in range(x-factor,x+factor+1):\n",
    "        if i>=0 and i<shape0:\n",
    "            for j in range(y-factor,y+factor+1):\n",
    "                if j>=0 and j<shape1:\n",
    "                    for k in range(z-factor,z+factor+1):\n",
    "                        if k>=0 and k<shape2:\n",
    "                            if not(i==x and j==y and k==z):\n",
    "                                index_list.append((i,j,k))\n",
    "    return index_list\n",
    "\n",
    "# return the upper link of a vertex (the neighbour which have a greater function than you)\n",
    "cdef list upper_link(int x, int y, int z,np.ndarray array):\n",
    "    cdef float value=array[x,y,z]\n",
    "    index_list=[]\n",
    "    cdef int i=0;\n",
    "    cdef int j=0;\n",
    "    cdef int k=0;\n",
    "    cdef int shape0=array.shape[0]\n",
    "    cdef int shape1=array.shape[1]\n",
    "    cdef int shape2=array.shape[2]\n",
    "    for i in range(x-1,x+2):\n",
    "        if i>=0 and i<shape0:\n",
    "            for j in range(y-1,y+2):\n",
    "                if j>=0 and j<shape1:\n",
    "                    for k in range(z-1,z+2):\n",
    "                        if k>=0 and k<shape2:\n",
    "                            if not(i==x and j==y and k==z):\n",
    "                                if (i==x or j==y or k==z):\n",
    "                                    if (array[i,j,k]>value):\n",
    "                                        index_list.append([i,j,k])\n",
    "                                    elif (array[i,j,k]==value):    \n",
    "                                        if i>x:\n",
    "                                            index_list.append([i,j,k])\n",
    "                                        elif i==x:\n",
    "                                            if j>y:\n",
    "                                                index_list.append([i,j,k])\n",
    "                                            elif j==y:\n",
    "                                                if k>z:\n",
    "                                                    index_list.append([i,j,k])\n",
    "\n",
    "    return index_list\n",
    "\n",
    "# return the value and the position of each cell of the array\n",
    "cdef list get_indexes(np.ndarray array):\n",
    "    cdef list indexes=[]\n",
    "    for i in range(0,array.shape[0]):\n",
    "        for j in range(0,array.shape[1]):\n",
    "            for k in range(0,array.shape[2]):\n",
    "                indexes.append([array[i,j,k],[i,j,k]])\n",
    "    return indexes    \n",
    "\n",
    "\n",
    "# main function which computes the join tree\n",
    "def hierarchical_segmentation(array):\n",
    "    indexes=get_indexes(array)\n",
    "    a=[d[0] for d in indexes]\n",
    "    b=[d[1][0] for d in indexes]\n",
    "    c=[d[1][1] for d in indexes]\n",
    "    d=[d[1][2] for d in indexes]\n",
    "#     sort the indexes by decreasing function value\n",
    "    lala=np.lexsort((d,c,b,a))\n",
    "    indexes=[[a[tmp],[b[tmp],c[tmp],d[tmp]]] for tmp in lala]\n",
    "    indexes=indexes[::-1]\n",
    "    topological_array=np.ones(array.shape)\n",
    "    topological_propagate=np.ones(array.shape)\n",
    "    topological_propagate[topological_propagate==1]=-1\n",
    "    topological_array[topological_array==1]=-1\n",
    "    count=0\n",
    "    maximum_set=set()\n",
    "    #we create a dictionary with each component and his parent\n",
    "    parent_dict={}\n",
    "    cdef int i=0;\n",
    "    print(\"Entering the loop\")\n",
    "    for i in range(len(indexes)):\n",
    "        value=indexes[i][0]\n",
    "        a=indexes[i][1][0]\n",
    "        b=indexes[i][1][1]\n",
    "        c=indexes[i][1][2]\n",
    "        \n",
    "#         for each vertex, we compute the upper link\n",
    "        index_list=upper_link(a,b,c,array)\n",
    "        tmp=[topological_propagate[tmp[0],tmp[1],tmp[2]] for tmp in index_list]\n",
    "        dict_key=set(tmp)\n",
    "#         if it's empty the we create a new component\n",
    "        if len(dict_key)==0:\n",
    "            topological_array[a,b,c]=count\n",
    "            topological_propagate[a,b,c]=count\n",
    "            maximum_set.add(count)\n",
    "            count+=1\n",
    "#         if if only contains one component (regular point) then we add the point to the existing component\n",
    "#         we don't create a new one\n",
    "        elif len(dict_key)==1:\n",
    "            topological_array[a,b,c]=topological_propagate[index_list[0][0],index_list[0][1],index_list[0][2]]\n",
    "            topological_propagate[a,b,c]=topological_propagate[index_list[0][0],index_list[0][1],index_list[0][2]]\n",
    "#         if the upper link contains more than two components then, we create a new one which englobes all the neighbours one\n",
    "        else:        \n",
    "            topological_propagate[a,b,c]=count\n",
    "            topological_array[a,b,c]=count\n",
    "            for j in dict_key:\n",
    "                parent_dict[j]=count\n",
    "                topological_propagate[topological_propagate==j]=count\n",
    "            count+=1\n",
    "    return topological_array,maximum_set,parent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how we mesure a component\n",
    "def set_w(a,topological_array,x,inverse_dict):\n",
    "    volume=float(np.sum(a[topological_array==x]))\n",
    "    return volume\n",
    "\n",
    "# for each component of the tree we compute w\n",
    "def compute_w(a,topological_array,inverse_dict):\n",
    "    w={}\n",
    "    for i in np.unique(topological_array):\n",
    "        w[i]=set_w(a,topological_array,i,inverse_dict)\n",
    "    return w\n",
    "\n",
    "# for all the components in the maximum set, we add the maximum component and his parent to the \n",
    "# list of the cylinder to remove\n",
    "def compute_sorted_x(maximum_set,w,parent_dict,topological_array,a):\n",
    "    sorted_x=[]\n",
    "    for i in maximum_set:\n",
    "        sorted_x=compute_sorted_x2(sorted_x,maximum_set,w,parent_dict,topological_array,i,a)\n",
    "    return sorted_x\n",
    "\n",
    "# compute the \n",
    "def compute_sorted_x2(sorted_x,maximum_set,w,parent_dict,topological_array,i,a):\n",
    "    if i in parent_dict:\n",
    "        j=parent_dict[i]\n",
    "        if i in w and j in w:\n",
    "                sorted_x.append(((i,j),w[i],np.sum(np.abs(np.mean(np.argwhere(topological_array==i),axis=0)-np.mean(np.argwhere(topological_array==j),axis=0)))))\n",
    "    return sorted_x\n",
    "\n",
    "# remove one branch from the join tree\n",
    "def single_tree_simplification(w,sorted_x,inverse_dict,maximum_set,topological_array,parent_dict,array):\n",
    "#     we sort all the maximum components we can remove\n",
    "    tmp1=[i[1] for i in sorted_x]\n",
    "    tmp2=[i[2] for i in sorted_x]\n",
    "    lala=np.lexsort((tmp2,tmp1))\n",
    "#     we delete the smallest one\n",
    "    maximum,saddle=sorted_x[lala[0]][0]\n",
    "    if maximum in maximum_set:\n",
    "        regular_2_list=[maximum]\n",
    "        maximum_set.remove(maximum)\n",
    "        #we replace all the place where this maximum component appears by his saddle parent\n",
    "        topological_array[topological_array==maximum]=saddle\n",
    "        #we compute the new value of the saddle\n",
    "        w[saddle]=set_w(a,topological_array,saddle,inverse_dict)\n",
    "        i=saddle\n",
    "        intersection=[(topological_array==j).sum() for j in inverse_dict[i]]\n",
    "        intersection_sum=np.count_nonzero(intersection)\n",
    "        #if his parent doesn't have any child anymore, then we put this saddle as a maximum\n",
    "        if intersection_sum==0:\n",
    "            maximum_set.add(i)\n",
    "            sorted_x=compute_sorted_x2(sorted_x,maximum_set,w,parent_dict,topological_array,i,a)\n",
    "        #if his parent only have on child anymore, then we merge his child with it and put it as a maximum\n",
    "        elif intersection_sum==1:\n",
    "            old_maximum=[j for j in inverse_dict[i] if ((topological_array==j).sum()!=0)][0]\n",
    "            if old_maximum in maximum_set:\n",
    "                maximum_set.remove(old_maximum)\n",
    "                topological_array[topological_array==old_maximum]=i\n",
    "                w[i]=set_w(a,topological_array,i,inverse_dict)\n",
    "                regular_2_list.append(old_maximum)\n",
    "                maximum_set.add(i)\n",
    "                sorted_x=compute_sorted_x2(sorted_x,maximum_set,w,parent_dict,topological_array,i,a)                \n",
    "                \n",
    "    sorted_x.pop(lala[0])\n",
    "    return w,sorted_x,inverse_dict,maximum_set,topological_array,parent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering the loop\n",
      "22.7015271187\n"
     ]
    }
   ],
   "source": [
    "a=whole_volume\n",
    "time1=time.time()\n",
    "# we compute the join tree\n",
    "topological_array,maximum_set,parent_dict=hierarchical_segmentation(a)\n",
    "time2=time.time()\n",
    "print(time2-time1)\n",
    "inverse_dict={}\n",
    "for i in parent_dict:\n",
    "    if parent_dict[i] not in inverse_dict:\n",
    "        inverse_dict[parent_dict[i]]=set([i])\n",
    "    else:\n",
    "        inverse_dict[parent_dict[i]].add(i)\n",
    "# compute the measure of each component\n",
    "w=compute_w(a,topological_array,inverse_dict)\n",
    "# add in a list all the maximum component that can be merged with their respective parent\n",
    "sorted_x=compute_sorted_x(maximum_set,w,parent_dict,topological_array,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of unique values', 19095, 'Number of maximum left', 9758)\n",
      "('Number of unique values', 18994, 'Number of maximum left', 9666)\n",
      "('Number of unique values', 18894, 'Number of maximum left', 9566)\n",
      "('Number of unique values', 18789, 'Number of maximum left', 9466)\n",
      "('Number of unique values', 18688, 'Number of maximum left', 9371)\n",
      "('Number of unique values', 18589, 'Number of maximum left', 9275)\n",
      "('Number of unique values', 18491, 'Number of maximum left', 9179)\n",
      "('Number of unique values', 18384, 'Number of maximum left', 9085)\n",
      "('Number of unique values', 18264, 'Number of maximum left', 8991)\n",
      "('Number of unique values', 18154, 'Number of maximum left', 8899)\n",
      "('Number of unique values', 18048, 'Number of maximum left', 8802)\n",
      "('Number of unique values', 17938, 'Number of maximum left', 8707)\n",
      "('Number of unique values', 17827, 'Number of maximum left', 8610)\n",
      "('Number of unique values', 17712, 'Number of maximum left', 8513)\n",
      "('Number of unique values', 17574, 'Number of maximum left', 8418)\n",
      "('Number of unique values', 17469, 'Number of maximum left', 8320)\n",
      "('Number of unique values', 17362, 'Number of maximum left', 8232)\n",
      "('Number of unique values', 17242, 'Number of maximum left', 8144)\n",
      "('Number of unique values', 17142, 'Number of maximum left', 8044)\n",
      "('Number of unique values', 17009, 'Number of maximum left', 7960)\n",
      "('Number of unique values', 16905, 'Number of maximum left', 7863)\n",
      "('Number of unique values', 16802, 'Number of maximum left', 7783)\n",
      "('Number of unique values', 16695, 'Number of maximum left', 7688)\n",
      "('Number of unique values', 16593, 'Number of maximum left', 7607)\n",
      "('Number of unique values', 16485, 'Number of maximum left', 7516)\n",
      "('Number of unique values', 16378, 'Number of maximum left', 7423)\n",
      "('Number of unique values', 16270, 'Number of maximum left', 7325)\n",
      "('Number of unique values', 16149, 'Number of maximum left', 7230)\n",
      "('Number of unique values', 16043, 'Number of maximum left', 7134)\n",
      "('Number of unique values', 15943, 'Number of maximum left', 7034)\n",
      "('Number of unique values', 15823, 'Number of maximum left', 6942)\n",
      "('Number of unique values', 15695, 'Number of maximum left', 6848)\n",
      "('Number of unique values', 15588, 'Number of maximum left', 6749)\n",
      "('Number of unique values', 15488, 'Number of maximum left', 6649)\n",
      "('Number of unique values', 15361, 'Number of maximum left', 6557)\n",
      "('Number of unique values', 15233, 'Number of maximum left', 6467)\n",
      "('Number of unique values', 15110, 'Number of maximum left', 6374)\n",
      "('Number of unique values', 15009, 'Number of maximum left', 6274)\n",
      "('Number of unique values', 14889, 'Number of maximum left', 6180)\n",
      "('Number of unique values', 14764, 'Number of maximum left', 6094)\n",
      "('Number of unique values', 14631, 'Number of maximum left', 6007)\n",
      "('Number of unique values', 14516, 'Number of maximum left', 5908)\n",
      "('Number of unique values', 14400, 'Number of maximum left', 5813)\n",
      "('Number of unique values', 14267, 'Number of maximum left', 5727)\n",
      "('Number of unique values', 14128, 'Number of maximum left', 5635)\n",
      "('Number of unique values', 14027, 'Number of maximum left', 5535)\n",
      "('Number of unique values', 13901, 'Number of maximum left', 5453)\n",
      "('Number of unique values', 13783, 'Number of maximum left', 5368)\n",
      "('Number of unique values', 13675, 'Number of maximum left', 5283)\n",
      "('Number of unique values', 13557, 'Number of maximum left', 5209)\n",
      "('Number of unique values', 13449, 'Number of maximum left', 5121)\n",
      "('Number of unique values', 13357, 'Number of maximum left', 5048)\n",
      "('Number of unique values', 13264, 'Number of maximum left', 4968)\n",
      "('Number of unique values', 13160, 'Number of maximum left', 4883)\n",
      "('Number of unique values', 13058, 'Number of maximum left', 4798)\n",
      "('Number of unique values', 12973, 'Number of maximum left', 4724)\n",
      "('Number of unique values', 12867, 'Number of maximum left', 4639)\n",
      "('Number of unique values', 12769, 'Number of maximum left', 4563)\n",
      "('Number of unique values', 12678, 'Number of maximum left', 4488)\n",
      "('Number of unique values', 12576, 'Number of maximum left', 4402)\n",
      "('Number of unique values', 12473, 'Number of maximum left', 4311)\n",
      "('Number of unique values', 12376, 'Number of maximum left', 4228)\n",
      "('Number of unique values', 12270, 'Number of maximum left', 4142)\n",
      "('Number of unique values', 12159, 'Number of maximum left', 4057)\n",
      "('Number of unique values', 12052, 'Number of maximum left', 3978)\n",
      "('Number of unique values', 11958, 'Number of maximum left', 3904)\n",
      "('Number of unique values', 11872, 'Number of maximum left', 3833)\n",
      "('Number of unique values', 11780, 'Number of maximum left', 3761)\n",
      "('Number of unique values', 11680, 'Number of maximum left', 3683)\n",
      "('Number of unique values', 11595, 'Number of maximum left', 3616)\n",
      "('Number of unique values', 11501, 'Number of maximum left', 3545)\n",
      "('Number of unique values', 11422, 'Number of maximum left', 3485)\n",
      "('Number of unique values', 11330, 'Number of maximum left', 3413)\n",
      "('Number of unique values', 11231, 'Number of maximum left', 3334)\n",
      "('Number of unique values', 11121, 'Number of maximum left', 3257)\n",
      "('Number of unique values', 11018, 'Number of maximum left', 3171)\n",
      "('Number of unique values', 10921, 'Number of maximum left', 3093)\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "n=2\n",
    "display_set=set([])\n",
    "# until n-1 branches are left\n",
    "while len(np.unique(topological_array))>2 and len(maximum_set)>n-1:\n",
    "# while count<10:\n",
    "    lala=topological_array.copy()\n",
    "    w_save=w.copy()\n",
    "    max_save=maximum_set.copy()\n",
    "#     we remove some branch at each time\n",
    "    w,sorted_x,inverse_dict,maximum_set,topological_array,parent_dict=single_tree_simplification(w,sorted_x,inverse_dict,maximum_set,topological_array,parent_dict,a)\n",
    "    count+=1\n",
    "    if count%100==0 :\n",
    "        print(\"Number of unique values\",len(np.unique(topological_array)),\"Number of maximum left\",len(maximum_set))\n",
    "\n",
    "    elif len(maximum_set)<=10 and len(maximum_set) not in display_set:\n",
    "        display_set.add(len(maximum_set))\n",
    "        #display the segmentation\n",
    "        bb=np.zeros(topological_array.shape)\n",
    "        for i in range(topological_array.shape[0]):\n",
    "            for j in range(topological_array.shape[1]):\n",
    "                for k in range(topological_array.shape[2]):\n",
    "                    if topological_array[i,j,k] in maximum_set:\n",
    "                        bb[i,j,k]=1\n",
    "        fig, ax = plt.subplots(figsize=(10,3))\n",
    "        fig.suptitle(len(maximum_set))\n",
    "        ax.imshow(np.mean(bb,axis=-1),aspect=\"auto\",)\n",
    "\n",
    "print(\"Max Nb \",len(maximum_set))\n",
    "print(\"Unique Nb \",len(np.unique(topological_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we display the branches that are left\n",
    "for i in max_save:\n",
    "    print(i)\n",
    "    q=np.zeros(topological_array.shape)\n",
    "    q[lala==i]=1\n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "    ax.imshow(np.mean(q,axis=-1),aspect=\"auto\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
